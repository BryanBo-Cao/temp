{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43afc6845b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mload_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convolutional Autoencoder\n",
    "Adapted from tutorial: https://github.com/pkmital/tensorflow_tutorials/\n",
    "'''\n",
    "\n",
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import ArtData\n",
    "from dist import cos_sim\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "STRIDE_SIZE = 2\n",
    "POOL_SIZE = 2\n",
    "POOL_STRIDE_SIZE = 2\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 600\n",
    "\n",
    "filters = [3, 32, 16]\n",
    "filter_maps = [2, 2]\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 16, 16, filters[0]])\n",
    "x_mean = tf.placeholder(tf.float32, [16, 16, filters[0]])\n",
    "\n",
    "# Encode images\n",
    "layer_in = x\n",
    "encode_params = []\n",
    "encode_shapes = []\n",
    "\n",
    "for i, output_len in enumerate(filters[1:]):\n",
    "\tencode_shapes.append(layer_in.get_shape().as_list())\n",
    "\tinput_len = layer_in.get_shape().as_list()[3]\n",
    "\n",
    "\tweights = tf.Variable(tf.random_normal([\n",
    "\t\tfilter_maps[i], filter_maps[i], input_len, output_len]))\n",
    "\tbias = tf.Variable(tf.random_normal([output_len]))\n",
    "\tencode_params.append(weights)\n",
    "\n",
    "\tlayer_in = tf.nn.sigmoid(tf.add(tf.nn.conv2d(\n",
    "\t\tlayer_in, weights, strides=[1, STRIDE_SIZE, STRIDE_SIZE, 1], padding='SAME'), bias))\n",
    "\n",
    "\t# layer_in = tf.nn.max_pool(layer_in, ksize=[1, POOL_SIZE, POOL_SIZE, 1],\n",
    "\t#                        strides=[1, POOL_STRIDE_SIZE, POOL_STRIDE_SIZE, 1], padding='SAME')\n",
    "\n",
    "print(layer_in)\n",
    "layer_in = tf.contrib.layers.flatten(layer_in)\n",
    "layer_in = tf.layers.dense(inputs=layer_in, units=256, activation=tf.nn.sigmoid)\n",
    "layer_in = tf.layers.dense(inputs=layer_in, units=128)\n",
    "\n",
    "# Latent representation of input images\n",
    "latent_rep_shape = layer_in.get_shape().as_list()\n",
    "#latent_rep = tf.reshape(layer_in, [-1, latent_rep_shape[1], latent_rep_shape[2], latent_rep_shape[3]])\n",
    "latent_rep = layer_in\n",
    "\n",
    "layer_in = tf.layers.dense(inputs=layer_in, units=256, activation=tf.nn.sigmoid)\n",
    "layer_in = tf.reshape(layer_in, (-1, 4, 4, 16))\n",
    "\n",
    "# Decode latent representation\n",
    "encode_params.reverse()\n",
    "encode_shapes.reverse()\n",
    "\n",
    "for i, shape in enumerate(encode_shapes):\n",
    "\tweights = encode_params[i]\n",
    "\tbias = tf.Variable(tf.zeros([weights.get_shape().as_list()[2]]))\n",
    "\n",
    "\t#layer_in = tf.image.resize_images(layer_in, size=(shape[1], shape[2]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "\tif i + 1 == len(encode_shapes):\n",
    "\t\tlayer_in = tf.add(tf.nn.conv2d_transpose(\n",
    "\t\t\tlayer_in, weights, tf.stack([tf.shape(x)[0], shape[1], shape[2], shape[3]]), strides=[1, STRIDE_SIZE, STRIDE_SIZE, 1], padding='SAME'), bias)\n",
    "\telse:\n",
    "\t\tlayer_in = tf.nn.sigmoid(tf.add(tf.nn.conv2d_transpose(\n",
    "\t\t\tlayer_in, weights, tf.stack([tf.shape(x)[0], shape[1], shape[2], shape[3]]), strides=[1, STRIDE_SIZE, STRIDE_SIZE, 1], padding='SAME'), bias))\n",
    "\n",
    "# Reconstruction of input images\n",
    "y = tf.nn.sigmoid(layer_in)\n",
    "\n",
    "#cost = tf.reduce_mean(tf.square(y - x))\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=layer_in))\n",
    "cost_2 = tf.reduce_sum(tf.pow(x - y, 2)) / tf.reduce_sum(tf.pow(x - x_mean, 2))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Initializing variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\tad = ArtData('../data/')\n",
    "\tad.load_images_and_pairs()\n",
    "\n",
    "\t# Extract training set\n",
    "\ttrain_data = ad.train_images.values()\n",
    "\tx_train = []\n",
    "\ty_train = []\n",
    "\tfor d in train_data:\n",
    "\t\tx_train.append(d[0])\n",
    "\t\ty_train.append(d[1])\n",
    "\tx_train = np.array(x_train)\n",
    "\ty_train = np.array(y_train)\n",
    "\n",
    "\t# Extract alpha set of pairs\n",
    "\talpha = ad.alpha_pairs\n",
    "\n",
    "\tfirst_images_same = []\n",
    "\tsecond_images_same = []\n",
    "\tcosine_sim_same = []\n",
    "\tpercent_correct_same = []\n",
    "\tfor d in alpha['same']:\n",
    "\t\tfirst_images_same.append(d[0])\n",
    "\t\tsecond_images_same.append(d[1])\n",
    "\t\tcosine_sim_same.append(d[2])\n",
    "\t\tpercent_correct_same.append(d[3])\n",
    "\tfirst_images_same = np.array(first_images_same)\n",
    "\tsecond_images_same = np.array(second_images_same)\n",
    "\tcosine_sim_same = np.array(cosine_sim_same)\n",
    "\tpercent_correct_same = np.array(percent_correct_same)\n",
    "\tprob_saying_same_same = percent_correct_same / 100.0\n",
    "\n",
    "\tcorr, p_value = scipy.stats.spearmanr(cosine_sim_same, percent_correct_same)\n",
    "\tprint('Same class correlation = {:.6f}'.format(corr))\n",
    "\n",
    "\tfirst_images_diff = []\n",
    "\tsecond_images_diff = []\n",
    "\tcosine_sim_diff = []\n",
    "\tneg_cosine_sim_diff = []\n",
    "\tpercent_correct_diff = []\n",
    "\tfor d in alpha['diff']:\n",
    "\t\tfirst_images_diff.append(d[0])\n",
    "\t\tsecond_images_diff.append(d[1])\n",
    "\t\tcosine_sim_diff.append(d[2])\n",
    "\t\tpercent_correct_diff.append(d[3])\n",
    "\t\tneg_cosine_sim_diff.append(d[6])\n",
    "\tfirst_images_diff = np.array(first_images_diff)\n",
    "\tsecond_images_diff = np.array(second_images_diff)\n",
    "\tcosine_sim_diff = np.array(cosine_sim_diff)\n",
    "\tneg_cosine_sim_diff = np.array(neg_cosine_sim_diff)\n",
    "\tpercent_correct_diff = np.array(percent_correct_diff)\n",
    "\tprob_saying_same_diff = 1.0 - (percent_correct_diff / 100.0)\n",
    "\n",
    "\tcorr, p_value = scipy.stats.spearmanr(cosine_sim_diff, percent_correct_diff)\n",
    "\tprint('Different class correlation = {:.6f}'.format(corr))\n",
    "\n",
    "\tall_cosine_sim = np.concatenate((cosine_sim_same, cosine_sim_diff), axis=0)\n",
    "\tall_prob_same = np.concatenate((prob_saying_same_same, prob_saying_same_diff), axis=0)\n",
    "\n",
    "\talpha_corr, p_value = scipy.stats.spearmanr(all_cosine_sim, all_prob_same)\n",
    "\tprint('All class correlation = {:.6f}'.format(alpha_corr))\n",
    "\n",
    "\t# plt.scatter(cosine_sim_same, prob_saying_same_same, color='b', label='Same Style')\n",
    "\t# plt.scatter(cosine_sim_diff, prob_saying_same_diff, color='r', label='Different Style')\n",
    "\t# plt.xlabel('Cosine Similarity')\n",
    "\t# plt.ylabel('Probability Human Judges as Same')\n",
    "\t# plt.title('Human Judgements vs. Cosine Similarity for Raw Pixel Art Image Styles (corr = {:.6f})'.format(corr))\n",
    "\t# plt.legend(loc='upper left')\n",
    "\t# plt.show()\n",
    "\n",
    "\t# Extract beta set of pairs\n",
    "\tbeta = ad.beta_pairs\n",
    "\n",
    "\tbeta_first_images_same = []\n",
    "\tbeta_second_images_same = []\n",
    "\tbeta_cosine_sim_same = []\n",
    "\tbeta_percent_correct_same = []\n",
    "\tfor d in beta['same']:\n",
    "\t\tbeta_first_images_same.append(d[0])\n",
    "\t\tbeta_second_images_same.append(d[1])\n",
    "\t\tbeta_cosine_sim_same.append(d[2])\n",
    "\t\tbeta_percent_correct_same.append(d[3])\n",
    "\tbeta_first_images_same = np.array(beta_first_images_same)\n",
    "\tbeta_second_images_same = np.array(beta_second_images_same)\n",
    "\tbeta_cosine_sim_same = np.array(beta_cosine_sim_same)\n",
    "\tbeta_percent_correct_same = np.array(beta_percent_correct_same)\n",
    "\tbeta_prob_saying_same_same = beta_percent_correct_same / 100.0\n",
    "\n",
    "\tcorr, p_value = scipy.stats.spearmanr(beta_cosine_sim_same, beta_percent_correct_same)\n",
    "\tprint('Beta Same class correlation = {:.6f}'.format(corr))\n",
    "\n",
    "\tbeta_first_images_diff = []\n",
    "\tbeta_second_images_diff = []\n",
    "\tbeta_cosine_sim_diff = []\n",
    "\tbeta_neg_cosine_sim_diff = []\n",
    "\tbeta_percent_correct_diff = []\n",
    "\tfor d in beta['diff']:\n",
    "\t\tbeta_first_images_diff.append(d[0])\n",
    "\t\tbeta_second_images_diff.append(d[1])\n",
    "\t\tbeta_cosine_sim_diff.append(d[2])\n",
    "\t\tbeta_percent_correct_diff.append(d[3])\n",
    "\t\tbeta_neg_cosine_sim_diff.append(d[6])\n",
    "\tbeta_first_images_diff = np.array(beta_first_images_diff)\n",
    "\tbeta_second_images_diff = np.array(beta_second_images_diff)\n",
    "\tbeta_cosine_sim_diff = np.array(beta_cosine_sim_diff)\n",
    "\tbeta_neg_cosine_sim_diff = np.array(beta_neg_cosine_sim_diff)\n",
    "\tbeta_percent_correct_diff = np.array(beta_percent_correct_diff)\n",
    "\tbeta_prob_saying_same_diff = 1.0 - (beta_percent_correct_diff / 100.0)\n",
    "\n",
    "\tcorr, p_value = scipy.stats.spearmanr(beta_cosine_sim_diff, beta_percent_correct_diff)\n",
    "\tprint('Beta Different class correlation = {:.6f}'.format(corr))\n",
    "\n",
    "\tbeta_all_cosine_sim = np.concatenate((beta_cosine_sim_same, beta_cosine_sim_diff), axis=0)\n",
    "\tbeta_all_prob_same = np.concatenate((beta_prob_saying_same_same, beta_prob_saying_same_diff), axis=0)\n",
    "\n",
    "\tbeta_corr, p_value = scipy.stats.spearmanr(beta_all_cosine_sim, beta_all_prob_same)\n",
    "\tprint('Beta all class correlation = {:.6f}'.format(beta_corr), '\\n')\n",
    "\n",
    "\treplications = 20\n",
    "\tcae_alpha_results = []\n",
    "\tcae_beta_results = []\n",
    "\tcae_mean_loss = []\n",
    "\tcae_cross_entropy_loss = []\n",
    "\tfor i in range(replications):\n",
    "\n",
    "\t\tsess.run(init)\n",
    "\n",
    "\t\tbest_results = {\n",
    "\t\t\t'all_corr': -1,\n",
    "\t\t\t'same_dist': None,\n",
    "\t\t\t'diff_dist': None,\n",
    "\t\t\t'beta_all_corr': -1,\n",
    "\t\t\t'mean_loss': 0,\n",
    "\t\t\t'cross_entropy_loss': 0\n",
    "\t\t}\n",
    "\n",
    "\t\tfor epoch in range(1, EPOCHS + 1):\n",
    "\t\t\tshuffle = np.random.permutation(len(y_train))\n",
    "\t\t\tx_train, y_train = x_train[shuffle], y_train[shuffle]\n",
    "\n",
    "\t\t\tfor i in range(0, len(y_train), BATCH_SIZE):\n",
    "\t\t\t\tx_train_mb, y_train_mb = x_train[i:i + BATCH_SIZE], y_train[i:i + BATCH_SIZE]\n",
    "\n",
    "\t\t\t\tsess.run(optimizer, feed_dict={x: x_train_mb, x_mean: np.mean(x_train_mb, axis=0)})\n",
    "\n",
    "\t\t\tc, c2 = sess.run([cost, cost_2], feed_dict={x: x_train, x_mean: np.mean(x_train, axis=0)})\n",
    "\n",
    "\t\t\tfirst_images_same_latent = sess.run(latent_rep, feed_dict={x: first_images_same})\n",
    "\t\t\tsecond_images_same_latent = sess.run(latent_rep, feed_dict={x: second_images_same})\n",
    "\t\t\tsame_dist = [cos_sim(a, b) for a, b in zip(first_images_same_latent, second_images_same_latent)]\n",
    "\n",
    "\t\t\tfirst_images_diff_latent = sess.run(latent_rep, feed_dict={x: first_images_diff})\n",
    "\t\t\tsecond_images_diff_latent = sess.run(latent_rep, feed_dict={x: second_images_diff})\n",
    "\t\t\tdiff_dist = [cos_sim(a, b) for a, b in zip(first_images_diff_latent, second_images_diff_latent)]\n",
    "\n",
    "\t\t\talpha_all_corr, _ = scipy.stats.spearmanr(np.concatenate((same_dist, diff_dist), axis=0), np.concatenate((prob_saying_same_same, prob_saying_same_diff), axis=0))\n",
    "\n",
    "\t\t\tbeta_first_images_same_latent = sess.run(latent_rep, feed_dict={x: beta_first_images_same})\n",
    "\t\t\tbeta_second_images_same_latent = sess.run(latent_rep, feed_dict={x: beta_second_images_same})\n",
    "\t\t\tbeta_same_dist = [cos_sim(a, b) for a, b in zip(beta_first_images_same_latent, beta_second_images_same_latent)]\n",
    "\n",
    "\t\t\tbeta_first_images_diff_latent = sess.run(latent_rep, feed_dict={x: beta_first_images_diff})\n",
    "\t\t\tbeta_second_images_diff_latent = sess.run(latent_rep, feed_dict={x: beta_second_images_diff})\n",
    "\t\t\tbeta_diff_dist = [cos_sim(a, b) for a, b in zip(beta_first_images_diff_latent, beta_second_images_diff_latent)]\n",
    "\n",
    "\t\t\tbeta_all_corr, _ = scipy.stats.spearmanr(np.concatenate((beta_same_dist, beta_diff_dist), axis=0), np.concatenate((beta_prob_saying_same_same, beta_prob_saying_same_diff), axis=0))\n",
    "\n",
    "\t\t\tif alpha_all_corr > best_results['all_corr']:\n",
    "\t\t\t\tbest_results['all_corr'] = alpha_all_corr\n",
    "\t\t\t\tbest_results['same_dist'] = np.copy(same_dist)\n",
    "\t\t\t\tprint(alpha_all_corr)\n",
    "\t\t\t\tbest_results['diff_dist'] = np.copy(diff_dist)\n",
    "\t\t\t\tbest_results['beta_all_corr'] = beta_all_corr\n",
    "\t\t\t\tbest_results['mean_loss'] = c2,\n",
    "\t\t\t\tbest_results['cross_entropy_loss'] = c\n",
    "\n",
    "\t\t\tif epoch % 10 == 0 or epoch == 1:\n",
    "\t\t\t\tc, c2 = sess.run([cost, cost_2], feed_dict={x: x_train, x_mean: np.mean(x_train, axis=0)})\n",
    "\n",
    "\t\t\t\tprint(\"Epoch \" + str(epoch) + \", Cost = \" + \"{:.6f}\".format(c) + \", Cost 2 = \" + \"{:.6f}\".format(c2))\n",
    "\n",
    "\t\t\t\tcorr, p_value = scipy.stats.spearmanr(same_dist, percent_correct_same)\n",
    "\n",
    "\t\t\t\tprint(\"Epoch \" + str(epoch) + \", Same Correlation = \" + \"{:.6f}\".format(corr))\n",
    "\n",
    "\t\t\t\tcorr, p_value = scipy.stats.spearmanr(diff_dist, percent_correct_diff)\n",
    "\n",
    "\t\t\t\tprint(\"Epoch \" + str(epoch) + \", Diff Correlation = \" + \"{:.6f}\".format(corr))\n",
    "\n",
    "\t\t\t\tprint(\"Epoch \" + str(epoch) + \", All Correlation = \" + \"{:.6f}\".format(alpha_all_corr) + '\\n')\n",
    "\n",
    "\t\tcae_alpha_results.append(best_results['all_corr'])\n",
    "\t\tcae_beta_results.append(best_results['beta_all_corr'])\n",
    "\t\tcae_mean_loss.append(best_results['mean_loss'])\n",
    "\t\tcae_cross_entropy_loss.append(best_results['cross_entropy_loss'])\n",
    "\n",
    "\tprint(alpha_corr)\n",
    "\tprint(beta_corr)\n",
    "\tprint(cae_alpha_results)\n",
    "\tprint(cae_beta_results)\n",
    "\tprint(cae_mean_loss)\n",
    "\tprint(cae_cross_entropy_loss)\n",
    "\n",
    "\t# plt.scatter(best_results['same_dist'], prob_saying_same_same, color='b', label='Same Style')\n",
    "\t# plt.scatter(best_results['diff_dist'], prob_saying_same_diff, color='r', label='Different Style')\n",
    "\t# plt.xlabel('Cosine Similarity')\n",
    "\t# plt.ylabel('Probability Human Judges as Same')\n",
    "\t# plt.title('Human Judgements vs. Cosine Similarity for Embedded Art Image Styles (corr = {:.6f})'.format(best_results['all_corr']))\n",
    "\t# plt.legend(loc='upper left')\n",
    "\t# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
